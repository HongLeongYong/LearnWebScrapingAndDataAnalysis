{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h4 class=\"card-title\">\n",
      "<a href=\"http://www.pycone.com/blogs#pablo\">Mac使用者</a>\n",
      "</h4>\n",
      "Mac使用者\n",
      "-----------------------\n",
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "resp = requests.get('http://blog.castman.net/py-scraping-analysis-book/ch2/blog/blog.html')\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "print(soup.find('h4'))\n",
    "# print(soup.h4)\n",
    "# 上面兩個 print 回傳的結果是一樣的\n",
    "print(soup.h4.a.text)\n",
    "\n",
    "print('-----------------------')\n",
    "main_titles = soup.find_all('h4')\n",
    "for title in main_titles:\n",
    "    print(title.a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------')\n",
    "# 取得所有 BLOG 主標題， 使用 class 屬性， 以下寫法皆相同\n",
    "\n",
    "# main_titles = soup.find_all('h4',{'class':'card-title'})\n",
    "# main_titles = soup.find_all('h4', class_='card-title')\n",
    "main_titles = soup.find_all('h4','card-title')\n",
    "for title in main_titles:\n",
    "    print(title.a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"card-description\" id=\"mac-p\">\n",
      "                                    在Mac環境下安裝Python與Sublime Text3<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>\n",
      "</p>\n",
      "<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(id = 'mac-p')) #使用 key=value 取得元件\n",
    "\n",
    "# 當key含特殊字元('-')時，使用dict取得元件\n",
    "# PS. soup.find(data-foo='mac-foo') 會導致 SyntaxError\n",
    "\n",
    "\n",
    "print(soup.find(attrs={\"data-foo\": \"mac-foo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "開發環境設定\n",
      "\n",
      "Mac使用者\n",
      "\n",
      "\n",
      "                                    在Mac環境下安裝Python與Sublime Text3 Read More \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    資料科學\n",
      "                                \n",
      "\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "\n",
      "                                     (1) 前言 Read More \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    資料科學\n",
      "                                \n",
      "\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "\n",
      "                                     (2) 套件安裝與啟動網頁爬蟲 Read More \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    資料科學\n",
      "                                \n",
      "\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "\n",
      "                                     (3) 解構並擷取網頁資料 Read More \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    資料科學\n",
      "                                \n",
      "\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "\n",
      "                                     (4) 擷取資料及下載圖片 Read More \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                    資料科學\n",
      "                                \n",
      "\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "\n",
      "                                     (5) 資料分析及展示 Read More \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取得各篇 blog 的所有文字,\n",
    "# 方法一， 使用.text（會包含許多換行符號）\n",
    "divs = soup.find_all('div','content')\n",
    "for div in divs:\n",
    "    print(div.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開發環境設定 Mac使用者 在Mac環境下安裝Python與Sublime Text3 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (1) 前言 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (2) 套件安裝與啟動網頁爬蟲 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (3) 解構並擷取網頁資料 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (4) 擷取資料及下載圖片 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (5) 資料分析及展示 Read More\n"
     ]
    }
   ],
   "source": [
    "# 取得各篇 blog 的所有文字\n",
    "# 方法二，使用TAG定位 （比上面那個好，有去掉空格）\n",
    "divs = soup.find_all('div','content')\n",
    "for div in divs:\n",
    "    print(div.h6.text.strip(), div.h4.a.text.strip(), div.p.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['開發環境設定', 'Mac使用者', '在Mac環境下安裝Python與Sublime Text3', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(1) 前言', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(2) 套件安裝與啟動網頁爬蟲', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(3) 解構並擷取網頁資料', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(4) 擷取資料及下載圖片', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(5) 資料分析及展示', 'Read More']\n"
     ]
    }
   ],
   "source": [
    "# 取得各篇 blog 的所有文字\n",
    "# 方法三 ， 使用 .stripped_strings\n",
    "divs = soup.find_all('div','content')\n",
    "for div in divs:\n",
    "    print([s for s in div.stripped_strings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
